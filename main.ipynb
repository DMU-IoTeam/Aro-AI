{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c92b55",
   "metadata": {},
   "source": [
    "# Fall Detection Full Pipeline (Training, Evaluation, Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7e8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 1. Import Libraries\n",
    "import os, cv2, glob\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05153b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 2. Pose Extractor\n",
    "class PoseExtractor:\n",
    "    def __init__(self, num_frames=16):\n",
    "        self.pose = mp.solutions.pose.Pose(static_image_mode=False)\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def extract_keypoints(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_idxs = np.linspace(0, total_frames - 1, self.num_frames).astype(int)\n",
    "        keypoints_sequence = []\n",
    "\n",
    "        for i in range(total_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i in frame_idxs:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                result = self.pose.process(frame_rgb)\n",
    "\n",
    "                if result.pose_landmarks:\n",
    "                    keypoints = []\n",
    "                    for lm in result.pose_landmarks.landmark:\n",
    "                        keypoints.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "                else:\n",
    "                    keypoints = [0] * (33 * 4)\n",
    "\n",
    "                keypoints_sequence.append(keypoints)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(keypoints_sequence) == 0:\n",
    "            print(f\"[â—] No keypoints extracted in: {video_path}\")\n",
    "            return None\n",
    "\n",
    "        while len(keypoints_sequence) < self.num_frames:\n",
    "            keypoints_sequence.append(keypoints_sequence[-1])\n",
    "\n",
    "        keypoints_sequence = np.array(keypoints_sequence)\n",
    "        velocity = np.diff(keypoints_sequence, axis=0, prepend=keypoints_sequence[0:1])\n",
    "        combined = np.concatenate([keypoints_sequence, velocity], axis=1)\n",
    "        return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a0d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 3. Custom Dataset\n",
    "class FallDataset(Dataset):\n",
    "    def __init__(self, npy_files, labels=None, return_fname=False):\n",
    "        self.files = npy_files\n",
    "        self.labels = labels\n",
    "        self.return_fname = return_fname\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        keypoints = np.load(self.files[idx])\n",
    "        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return keypoints, label\n",
    "        elif self.return_fname:\n",
    "            fname = os.path.basename(self.files[idx])\n",
    "            return keypoints, fname\n",
    "        else:\n",
    "            return keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d24e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 4. GRU Classifier\n",
    "class FallGRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size=264, hidden_size=64, num_layers=1, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389f1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 5. Training and Evaluation Functions\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        all_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "        all_labels += y.cpu().tolist()\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return total_loss / len(dataloader), acc, recall, f1\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            all_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "            all_labels += y.cpu().tolist()\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    return total_loss / len(dataloader), acc, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4517e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… 6. Prepare Data\n",
    "os.makedirs(\"data/npy\", exist_ok=True)\n",
    "\n",
    "fall_videos = glob.glob(\"data/video/Y/*/*/*.mp4\")\n",
    "not_fall_videos = glob.glob(\"data/video/N/N/*/*.mp4\")\n",
    "\n",
    "\n",
    "def get_npy_path_from_video_path(video_path):\n",
    "    fname = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    return os.path.join(\"data/npy\", f\"{fname}.npy\")\n",
    "\n",
    "extractor = PoseExtractor(num_frames=16)\n",
    "\n",
    "# ë‚™ìƒ ì˜ìƒ â†’ .npy ì €ì¥\n",
    "for video_path in fall_videos:\n",
    "    npy_path = get_npy_path_from_video_path(video_path)\n",
    "    if not os.path.exists(npy_path):\n",
    "        keypoints = extractor.extract_keypoints(video_path)\n",
    "        if keypoints is not None:\n",
    "            np.save(npy_path, keypoints)\n",
    "        else:\n",
    "            print(f\"[âš ï¸] ë‚™ìƒ ì‹¤íŒ¨: {video_path}\")\n",
    "\n",
    "# ì •ìƒ ì˜ìƒ â†’ .npy ì €ì¥\n",
    "for video_path in not_fall_videos:\n",
    "    npy_path = get_npy_path_from_video_path(video_path)\n",
    "    if not os.path.exists(npy_path):\n",
    "        keypoints = extractor.extract_keypoints(video_path)\n",
    "        if keypoints is not None:\n",
    "            np.save(npy_path, keypoints)\n",
    "        else:\n",
    "            print(f\"[âš ï¸] ì •ìƒ ì‹¤íŒ¨: {video_path}\")\n",
    "\n",
    "# í•™ìŠµì— ì‚¬ìš©ë  .npy ê²½ë¡œ ì„¤ì •\n",
    "fall_files = [get_npy_path_from_video_path(p) for p in fall_videos if os.path.exists(get_npy_path_from_video_path(p))]\n",
    "not_fall_files = [get_npy_path_from_video_path(p) for p in not_fall_videos if os.path.exists(get_npy_path_from_video_path(p))]\n",
    "\n",
    "\n",
    "all_files = fall_files + not_fall_files\n",
    "labels = [1]*len(fall_files) + [0]*len(not_fall_files)\n",
    "\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    all_files, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = FallDataset(train_files, labels=train_labels)\n",
    "val_dataset = FallDataset(val_files, labels=val_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aea2435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1746512812.797802  114718 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746512812.818717  114718 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.57      1.00      0.73         4\n",
      "\n",
      "    accuracy                           0.57         7\n",
      "   macro avg       0.29      0.50      0.36         7\n",
      "weighted avg       0.33      0.57      0.42         7\n",
      "\n",
      "\n",
      "ğŸ“˜ Epoch 1\n",
      "Train | Loss: 0.4662, Acc: 80.00%, Recall: 100.00%, F1: 88.89%\n",
      "Val   | Loss: 0.5669, Acc: 57.14%, Recall: 100.00%, F1: 72.73%\n",
      "âœ… Best model saved (F1: 72.73%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.57      1.00      0.73         4\n",
      "\n",
      "    accuracy                           0.57         7\n",
      "   macro avg       0.29      0.50      0.36         7\n",
      "weighted avg       0.33      0.57      0.42         7\n",
      "\n",
      "\n",
      "ğŸ“˜ Epoch 2\n",
      "Train | Loss: 0.3674, Acc: 80.00%, Recall: 100.00%, F1: 88.89%\n",
      "Val   | Loss: 0.6115, Acc: 57.14%, Recall: 100.00%, F1: 72.73%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.57      1.00      0.73         4\n",
      "\n",
      "    accuracy                           0.57         7\n",
      "   macro avg       0.29      0.50      0.36         7\n",
      "weighted avg       0.33      0.57      0.42         7\n",
      "\n",
      "\n",
      "ğŸ“˜ Epoch 3\n",
      "Train | Loss: 0.3654, Acc: 80.00%, Recall: 100.00%, F1: 88.89%\n",
      "Val   | Loss: 0.6530, Acc: 57.14%, Recall: 100.00%, F1: 72.73%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.57      1.00      0.73         4\n",
      "\n",
      "    accuracy                           0.57         7\n",
      "   macro avg       0.29      0.50      0.36         7\n",
      "weighted avg       0.33      0.57      0.42         7\n",
      "\n",
      "\n",
      "ğŸ“˜ Epoch 4\n",
      "Train | Loss: 0.3708, Acc: 80.00%, Recall: 100.00%, F1: 88.89%\n",
      "Val   | Loss: 0.6791, Acc: 57.14%, Recall: 100.00%, F1: 72.73%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.57      1.00      0.73         4\n",
      "\n",
      "    accuracy                           0.57         7\n",
      "   macro avg       0.29      0.50      0.36         7\n",
      "weighted avg       0.33      0.57      0.42         7\n",
      "\n",
      "\n",
      "ğŸ“˜ Epoch 5\n",
      "Train | Loss: 0.8974, Acc: 80.00%, Recall: 100.00%, F1: 88.89%\n",
      "Val   | Loss: 0.6855, Acc: 57.14%, Recall: 100.00%, F1: 72.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# âœ… 7. Train Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FallGRUClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_f1 = 0.0\n",
    "save_path = 'fall_gru_best.pt'\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc, train_recall, train_f1 = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_recall, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"\\nğŸ“˜ Epoch {epoch+1}\")\n",
    "    print(f\"Train | Loss: {train_loss:.4f}, Acc: {train_acc:.2%}, Recall: {train_recall:.2%}, F1: {train_f1:.2%}\")\n",
    "    print(f\"Val   | Loss: {val_loss:.4f}, Acc: {val_acc:.2%}, Recall: {val_recall:.2%}, F1: {val_f1:.2%}\")\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"âœ… Best model saved (F1: {val_f1:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88be3318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746512813.257539  114763 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746512813.288548  114778 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746512813.457657  114764 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì˜ˆì¸¡ ê²°ê³¼: ë‚™ìƒ\n",
      "â†’ í´ë˜ìŠ¤ í™•ë¥  (ì •ìƒ, ë‚™ìƒ): [0.16869348 0.8313066 ]\n"
     ]
    }
   ],
   "source": [
    "# âœ… 8. Inference (from .mp4)\n",
    "def infer_video_file(model, video_path, device, num_frames=16):\n",
    "    extractor = PoseExtractor(num_frames)\n",
    "    keypoints = extractor.extract_keypoints(video_path)\n",
    "    if keypoints is None:\n",
    "        print(f\"[âš ï¸] ê´€ì ˆì´ ê°ì§€ë˜ì§€ ì•Šì•„ ì¶”ë¡  ë¶ˆê°€: {video_path}\")\n",
    "        return None\n",
    "    x = torch.tensor(keypoints, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        prob = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "        pred_class = np.argmax(prob)\n",
    "        return pred_class, prob\n",
    "    \n",
    "\n",
    "\n",
    "model = FallGRUClassifier().to(device)\n",
    "model.load_state_dict(torch.load('fall_gru_best.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "#video_path = \"data/video/Y/FY/00003_H_A_FY_C1/00003_H_A_FY_C1.mp4\"\n",
    "video_path = \"data/video/N/N/00047_H_A_N_C1/00047_H_A_N_C1.mp4\"\n",
    "pred, prob = infer_video_file(model, video_path, device)\n",
    "\n",
    "if pred is not None:\n",
    "    print(f\"ğŸ“ ì˜ˆì¸¡ ê²°ê³¼: {'ë‚™ìƒ' if pred == 1 else 'ì •ìƒ'}\")\n",
    "    print(f\"â†’ í´ë˜ìŠ¤ í™•ë¥  (ì •ìƒ, ë‚™ìƒ): {prob}\")\n",
    "else:\n",
    "    print(\"âš ï¸ ê´€ì ˆ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ì¶”ë¡  ë¶ˆê°€\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c84a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 9. Real-Time Inference\n",
    "def realtime_fall_detection(model, device, num_frames=16):\n",
    "    pose = mp.solutions.pose.Pose(static_image_mode=False)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    keypoints_sequence = []\n",
    "    print(\"ğŸŸ¢ ì‹¤ì‹œê°„ ë‚™ìƒ ê°ì§€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\")\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"â— í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨\")\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        result = pose.process(frame_rgb)\n",
    "        if result.pose_landmarks:\n",
    "            keypoints = [val for lm in result.pose_landmarks.landmark for val in (lm.x, lm.y, lm.z, lm.visibility)]\n",
    "        else:\n",
    "            keypoints = [0] * 132\n",
    "        keypoints_sequence.append(keypoints)\n",
    "        if len(keypoints_sequence) == num_frames:\n",
    "            input_tensor = torch.tensor([keypoints_sequence], dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                pred = torch.argmax(output, dim=1).item()\n",
    "                print(\"ğŸ“ ì˜ˆì¸¡ ê²°ê³¼:\", \"ë‚™ìƒ\" if pred == 1 else \"ì •ìƒ\")\n",
    "            keypoints_sequence = []\n",
    "        cv2.imshow(\"Real-time Fall Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a85e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746512843.597399  114870 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746512843.632767  114881 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œëœ í”„ë ˆì„ ìˆ˜: 16\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "def visualize_pose_on_video_debug(video_path, save_dir=\"vis_frames\", num_frames=16):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"âŒ Failed to open video: {video_path}\")\n",
    "        return\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idxs = np.linspace(0, total_frames - 1, num_frames).astype(int)\n",
    "\n",
    "    pose = mp.solutions.pose.Pose()\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_style = mp.solutions.drawing_styles\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"âš ï¸ Frame {i} read failed\")\n",
    "            break\n",
    "\n",
    "        if i in frame_idxs:\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            result = pose.process(rgb)\n",
    "\n",
    "            if result.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, result.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_style.get_default_pose_landmarks_style()\n",
    "                )\n",
    "            else:\n",
    "                print(f\"âš ï¸ No landmarks detected in frame {i}\")\n",
    "\n",
    "            save_path = os.path.join(save_dir, f\"frame_{idx:02d}.jpg\")\n",
    "            success = cv2.imwrite(save_path, frame)\n",
    "            if not success:\n",
    "                print(f\"âŒ Failed to save: {save_path}\")\n",
    "            idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œëœ í”„ë ˆì„ ìˆ˜: {idx}\")\n",
    "\n",
    "\n",
    "\n",
    "visualize_pose_on_video_debug(\"data/video/N/N/00047_H_A_N_C1/00047_H_A_N_C1.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79a493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
